---
title: "STAT 108: Lab 7"
author: "Tim Lanthier"
date: "3/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 7: Logistic Regression
```{r message = FALSE}
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(knitr)
```

In this lab we will be investigating Spotify song attribute data from 2017 from a single user. The raw dataset can be found on the Github Repository.

```{r}
spotify <- read.csv('data/spotify.csv')
glimpse(spotify)
```


## Data Prep & Modeling

We will be creating a model to predict whether or not the user likes a song or not. Here, the response variable is `target` where 1 means the user liked the song, 0 means they did not like the song. Note that in the glimpse above, `target` is an integer value which we need to change to a factor. We also will need to change `key` to a factor.

```{r}
spotify <- spotify %>%
  mutate(target = as.factor(target), 
         key = ifelse(key == 2, 'D', ifelse(key == 3, 'D#', 'Other')))
```

```{r}
ggplot(data = spotify, aes(x = target)) +
  geom_bar() +
  labs(x = 'Target', title = 'Distribution of Target')

ggplot(data = spotify, aes(x = key)) +
  geom_bar() +
  labs(x = 'Key', title = 'Distribution of Key')

ggplot(data = spotify, aes(x = key, fill = target)) +
  geom_bar(position = 'fill') +
  labs(x = 'Key', y = 'Proportion', title = 'Key vs Target')
```
According to the above plot, it looks like the user appears to dislike songs in D# shown by a approximately 30\% of the songs in D# being liked by the user. Meanwhile the user likes approximately 60\% of the songs in D and around 50% of the songs in other keys. Now we will build a logistic regression model to predict `target` using `acousticness`, `danceability`, `duration_ms`, `instrumentalness`, `loudness`, `speechiness`, and `valence` as our predictors.

```{r}
model <- glm(target ~ acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence,  
             data = spotify, 
             family = 'binomial')
tidy(model, conf.int=TRUE) %>%
  kable(digits = 6, format = 'markdown')
```
Now we will consider adding `key` to our model. Consider the model `model_key` shown below which is our original model but including `key` as an additional predictor.

```{r}
model_key <- glm(target ~ key + acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence,  
             data = spotify, 
             family = 'binomial')
```
We will now conduct a drop-in-deviance test between `model` and `model_key` to check whether we should include `key` in our final model. Note that in `model_key`, we have an additional 2 terms: `keyD#` and `keyOther`. So if we let $\beta_1$ and $\beta_2$ be the coefficients for `keyD#` and `keyOther` in our model including `key`, then we have the null hypothesis that $\beta_1 = \beta_2 = 0$ and the alternate hypothesis that at least one of $\beta_1$ and $\beta_2$ is nonzero. Now we may conduct a drop-in-deviance test.

```{r}
dev_model <- glance(model)$deviance
dev_model_key <- glance(model_key)$deviance
test_stat <- dev_model - dev_model_key
```

Now as we noted earlier, we have an additional 2 parameters in `model_key` compared to the original model. So we will be checking the probability that $\chi^2$ is greater than our test statistic where $\chi^2$ follows a $chi^2$-distribution with 2 degrees of freedom.

```{r}
1-pchisq(test_stat,2)
```

The p-value from our drop-in-deviance test is shown. So at the 0.01 significance level, since our p-value is less than 0.01, we have sufficient evidence to reject the null hypothesis that $\beta_1 = \beta_2 = 0$. So we are confident that at least one of $\beta_1$ and $\beta_2$ is nonzero which would suggest that we should include `key` in our model.

