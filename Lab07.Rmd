---
title: 'STAT 108: Lab 7'
author: "Tim Lanthier"
date: "3/2/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 7: Logistic Regression

Github: https://github.com/talanthier/lab-07
```{r message = FALSE}
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(knitr)
```

In this lab we will be investigating Spotify song attribute data from 2017 from a single user. The raw dataset can be found on the Github Repository.

```{r}
spotify <- read.csv('data/spotify.csv')
glimpse(spotify)
```


## Data Prep & Modeling

We will be creating a model to predict whether or not the user likes a song or not. Here, the response variable is `target` where 1 means the user liked the song, 0 means they did not like the song. Note that in the glimpse above, `target` is an integer value which we need to change to a factor. We also will need to change `key` to a factor.

```{r}
spotify <- spotify %>%
  mutate(target = as.factor(target), 
         key = ifelse(key == 2, 'D', ifelse(key == 3, 'D#', 'Other')))
```

```{r}
ggplot(data = spotify, aes(x = target)) +
  geom_bar() +
  labs(x = 'Target', title = 'Distribution of Target')

ggplot(data = spotify, aes(x = key)) +
  geom_bar() +
  labs(x = 'Key', title = 'Distribution of Key')

ggplot(data = spotify, aes(x = key, fill = target)) +
  geom_bar(position = 'fill') +
  labs(x = 'Key', y = 'Proportion', title = 'Key vs Target')
```
According to the above plot, it looks like the user appears to dislike songs in D# shown by a approximately 30\% of the songs in D# being liked by the user. Meanwhile the user likes approximately 60\% of the songs in D and around 50% of the songs in other keys. Now we will build a logistic regression model to predict `target` using `acousticness`, `danceability`, `duration_ms`, `instrumentalness`, `loudness`, `speechiness`, and `valence` as our predictors.

```{r}
model <- glm(target ~ acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence,  
             data = spotify, 
             family = 'binomial')
tidy(model, conf.int=TRUE) %>%
  kable(digits = 6, format = 'markdown')
```
Now we will consider adding `key` to our model. Consider the model `model_key` shown below which is our original model but including `key` as an additional predictor.

```{r}
model_key <- glm(target ~ key + acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence,  
             data = spotify, 
             family = 'binomial')
```
We will now conduct a drop-in-deviance test between `model` and `model_key` to check whether we should include `key` in our final model. Note that in `model_key`, we have an additional 2 terms: `keyD#` and `keyOther`. So if we let $\beta_1$ and $\beta_2$ be the coefficients for `keyD#` and `keyOther` in our model including `key`, then we have the null hypothesis that $\beta_1 = \beta_2 = 0$ and the alternate hypothesis that at least one of $\beta_1$ and $\beta_2$ is nonzero. Now we may conduct a drop-in-deviance test.

```{r}
dev_model <- glance(model)$deviance
dev_model_key <- glance(model_key)$deviance
test_stat <- dev_model - dev_model_key
```

Now as we noted earlier, we have an additional 2 parameters in `model_key` compared to the original model. So we will be checking the probability that $\chi^2$ is greater than our test statistic where $\chi^2$ follows a $chi^2$-distribution with 2 degrees of freedom.

```{r}
1-pchisq(test_stat,2)
```

The p-value from our drop-in-deviance test is shown. So at the 0.01 significance level, since our p-value is less than 0.01, we have sufficient evidence to reject the null hypothesis that $\beta_1 = \beta_2 = 0$. So we are confident that at least one of $\beta_1$ and $\beta_2$ is nonzero. This means that `key` will have some predictive power in our model. Thus we should include `key` in our model. So for the remainder of the lab, we will be using `model_key` shown below.

```{r}
tidy(model_key, conf.int = TRUE) %>%
  kable(digits = 3)
```

According to the model output, looking at the coefficient for `keyD#`, a song will have a log odds of being liked by the user that is $e^{-1.073} = 0.342$ times the log odds of a song with the same characteristics with the only difference that it is in the D key.

## Checking Assumptions

We will now check the assumptions for our model. We will start by checking the binned residuals and the predicted probabilities for our model.

```{r}
spotify_aug <- augment(model_key, type.predict = 'response')

arm::binnedplot(x = spotify_aug$.fitted, y = spotify_aug$.resid,
                xlab = "Predicted Probabilities", 
                main = "Binned Residual vs. Predicted Values", 
                col.int = FALSE)
```
We will also examine the binned residual plots for `loudness`.
```{r}
arm::binnedplot(x = spotify_aug$loudness, y = spotify_aug$.resid,
                xlab = "Loudness", 
                main = "Binned Residual vs. Loudness", 
                col.int = FALSE)
```
Now consider the residuals in each category of `key`.

```{r}
spotify_aug %>%
  group_by(key) %>%
  summarise(mean_resid = mean(.resid))
```

Now that we have the above plots and table, we can see that the linearity assumption is not satisfied. Starting with the binned residual plot of Binned residuals versus predicted probabilities, we see a clear slightly V-shaped pattern. Looking at the binned residual plot for `loudness`, we also see a clear pattern. Songs with either low or high loudness on average have residuals below zero. Since we do not have a random scatter in our binned residual plots, we conclude that the linearity assumption is not satisfied. That being said, the mean residuals across the groups for `key` are pretty close to one another (all within 0.1 of 0).

## Model Assessment and Prediction

```{r}
(roc_curve <- ggplot(spotify_aug, aes(d = as.numeric(target) - 1,m = .fitted)) +
  geom_roc(n.cuts = 10, labelround = 3) + 
  geom_abline(intercept = 0) + 
  labs(x = 'False Positive Rate', y = 'True Positive rate'))

calc_auc(roc_curve)$AUC
```
So we have an AUC of 0.714. Since an AUC of close to 1 would indicate a very good fit and an AUC close to 0.5 would indicate a poor fit, I would say that our model does just an okay job of differentiating between the songs the user likes and doesn't like. Looking at are ROC curve, it does not get particularly close to the ideal case where we have a True Positive rate very close to 1 and false positive rate very close to 0.

In this case, we are predicting whether the user likes or dislikes a song. In the context of this problem, a false positive means we classified a song which the user dislikes as a song we think he would like. A false negative would be when we classify a song that the user actually likes as a song that he should dislike. So in the context of this problem, neither are particularly dangerous. So we should choose a threshold value on the ROC curve closest fo the top left corner (high true positive rate, low false positive rate). Looking at our ROC curve, our optimal threshold might be between 0.58 and 0.521. So we might choose a threshold of 0.55.

For the rest of the lab we will use this threshold value of 0.55.

```{r}
threshold <- 0.55

spotify_aug %>%
  mutate(target_pred = if_else(.fitted > threshold, "1: Liked", "0: Disliked")) %>%
  group_by(target, target_pred) %>%
  summarise(n = n()) %>%
  kable(format="markdown")
```
The confusion matrix for our model with this threshold value is shown above. Hence we have a specificity of
\[\text{specificity} = \frac{581}{439 + 581} = 0.5696\]
and sensitivity of
\[\text{sensitivity} = \frac{784}{784+213} = 0.7864\]
Hence we get a false positive rate of 
\[1 - \text{sensitivity} = 0.2136\]

In total, we misclassified $213 + 581 = 794$ songs out of a total of $784 +213 + 439 + 581 = 2017$ songs. So we have a misclassification rate of
$\frac{794}{2017} = 0.394$.

