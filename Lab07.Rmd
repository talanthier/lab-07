---
title: "STAT 108: Lab 7"
author: "Tim Lanthier"
date: "3/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 7: Logistic Regression
```{r message = FALSE}
library(tidyverse)
library(broom)
library(pROC)
library(plotROC)
library(knitr)
```

In this lab we will be investigating Spotify song attribute data from 2017 from a single user. The raw dataset can be found on the Github Repository.

```{r}
spotify <- read.csv('data/spotify.csv')
glimpse(spotify)
```


## Data Prep & Modeling

We will be creating a model to predict whether or not the user likes a song or not. Here, the response variable is `target` where 1 means the user liked the song, 0 means they did not like the song. Note that in the glimpse above, `target` is an integer value which we need to change to a factor. We also will need to change `key` to a factor.

```{r}
spotify <- spotify %>%
  mutate(target = as.factor(target), 
         key = ifelse(key == 2, 'D', ifelse(key == 3, 'D#', 'Other')))
```

```{r}
ggplot(data = spotify, aes(x = target)) +
  geom_bar() +
  labs(x = 'Target', title = 'Distribution of Target')

ggplot(data = spotify, aes(x = key)) +
  geom_bar() +
  labs(x = 'Key', title = 'Distribution of Key')

ggplot(data = spotify, aes(x = key, fill = target)) +
  geom_bar(position = 'fill') +
  labs(x = 'Key', y = 'Proportion', title = 'Key vs Target')
```
According to the above plot, it looks like the user appears to dislike songs in D# shown by a approximately 30\% of the songs in D# being liked by the user. Meanwhile the user likes approximately 60\% of the songs in D and around 50% of the songs in other keys. Now we will build a logistic regression model to predict `target` using `acousticness`, `danceability`, `duration_ms`, `instrumentalness`, `loudness`, `speechiness`, and `valence` as our predictors.

```{r}
model <- glm(target ~ acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence,  
             data = spotify, 
             family = 'binomial')
tidy(model, conf.int=TRUE) %>%
  kable(digits = 6, format = 'markdown')
```
Now we will consider adding `key` to our model. Consider the model `model_key` shown below which is our original model but including `key` as an additional predictor.

```{r}
model_key <- glm(target ~ key + acousticness + danceability + duration_ms + instrumentalness + loudness + speechiness + valence,  
             data = spotify, 
             family = 'binomial')
```
We will now conduct a drop-in-deviance test between `model` and `model_key` to check whether we should include `key` in our final model. Note that in `model_key`, we have an additional 2 terms: `keyD#` and `keyOther`. So if we let $\beta_1$ and $\beta_2$ be the coefficients for `keyD#` and `keyOther` in our model including `key`, then we have the null hypothesis that $\beta_1 = \beta_2 = 0$ and the alternate hypothesis that at least one of $\beta_1$ and $\beta_2$ is nonzero. Now we may conduct a drop-in-deviance test.

```{r}
dev_model <- glance(model)$deviance
dev_model_key <- glance(model_key)$deviance
test_stat <- dev_model - dev_model_key
```

Now as we noted earlier, we have an additional 2 parameters in `model_key` compared to the original model. So we will be checking the probability that $\chi^2$ is greater than our test statistic where $\chi^2$ follows a $chi^2$-distribution with 2 degrees of freedom.

```{r}
1-pchisq(test_stat,2)
```

The p-value from our drop-in-deviance test is shown. So at the 0.01 significance level, since our p-value is less than 0.01, we have sufficient evidence to reject the null hypothesis that $\beta_1 = \beta_2 = 0$. So we are confident that at least one of $\beta_1$ and $\beta_2$ is nonzero. This means that `key` will have some predictive power in our model. Thus we should include `key` in our model. So for the remainder of the lab, we will be using `model_key` shown below.

```{r}
tidy(model_key, conf.int = TRUE) %>%
  kable(digits = 3)
```

According to the model output, looking at the coefficient for `keyD#`, a song will have a log odds of being liked by the user that is $e^{-1.073} = 0.342$ times the log odds of a song with the same characteristics with the only difference that it is in the D key.

## Checking Assumptions

We will now check the assumptions for our model. We will start by checking the binned residuals and the predicted probabilities for our model.

```{r}
spotify_aug <- augment(model_key, type.predict = 'response')

arm::binnedplot(x = spotify_aug$.fitted, y = spotify_aug$.resid,
                xlab = "Predicted Probabilities", 
                main = "Binned Residual vs. Predicted Values", 
                col.int = FALSE)
```
We will also examine the binned residual plots for `loudness`.
```{r}
arm::binnedplot(x = spotify_aug$loudness, y = spotify_aug$.resid,
                xlab = "Loudness", 
                main = "Binned Residual vs. Loudness", 
                col.int = FALSE)
```
Now consider the residuals in each category of `key`.

```{r}
spotify_aug %>%
  group_by(key) %>%
  summarise(mean_resid = mean(.resid))
```

Now that we have the above plots and table, we can see that the linearity assumption is not satisfied. Starting with the binned residual plot of Binned residuals versus predicted probabilities, we see a clear slightly V-shaped pattern. Looking at the binned residual plot for `loudness`, we also see a clear pattern. Songs with either low or high loudness on average have residuals below zero. Since we do not have a random scatter in our binned residual plots, we conclude that the linearity assumption is not satisfied. That being said, the mean residuals across the groups for `key` are pretty close to one another (all within 0.1 of 0).

## Model Assessment and Prediction


